import numpy as np
import os
import json
import pandas as pd
# import utilities as utl
# from body_parts import Body_Parts
# import itertools
import numpy as np
import pandas as pd
import os
# import sys
# from matplotlib import pyplot as plt
# import tensorflow as tf
# from tensorflow import keras



#load all the data points from json generated by openpose
def read_pose_from_json(path_to_json, num_sequence):
    # Load keypoint data from JSON output
    column_names = ['frames', 'label']
    # Paths - should be the folder where Open Pose JSON output was stored
    # Import Json files, pos_json = position JSON
    # json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]
    json_files = find_json_files(path_to_json)
    #need to sort the files so that loaded frames maintains sequence
    json_files.sort()
    #print('Found: ', len(json_files), 'json keypoint frame files')
    count = 0
    # instanciate dataframes
    frames = []
    data = []
    label='fatigued'
    # Loop through all json files in output directory
    for file in json_files:
        #print(file)
        #temp_df = json.load(open(path_to_json+file))
        with open(file) as f:
            print(f)
            data = json.load(f)
            if len(data['people']) > 0:
                data = np.array(data['people'][0]['pose_keypoints_2d']) #.reshape(-1, 3)
                frames.append(data)
            else:
                continue
    #include the first 300 frames of sequences
    frames = frames[:num_sequence]
    frames = np.array(frames).flatten()
    return frames


# create dataframe from available keypoint json files generated using OpenPose.
# param path is the root path of the dataset
def create_dataframe(path, num_user, num_session, activity_instances, camera_view, num_sequence):

    #path_string = f'output_json_folder/{user_id}/{session_id}/{activity_instance}/gait_frontview/'
    # session id starts from 1
    keypoint_sequences = []
    labels = []
    classnums = []
    for user in range(1, num_user + 1):
        for session in range(1, num_session+1):
            for ai in range(len(activity_instances)):
                user_id = 'user_'+str(user)
                session_id = 'session_' + str(session)
                path_string = f'output_json_folder/{user_id}/{session_id}/{activity_instances[ai]}/{camera_view}/'
                path_to_keypoints = path + path_string
                #print(path_to_keypoints)
                if os.path.isdir(path_to_keypoints):
                    # the path exists. try to load the keypoints
                    keypoint_frame = read_pose_from_json(path_to_keypoints, num_sequence)
                    keypoint_sequences.append(keypoint_frame)
                    labels.append(activity_instances[ai])
                    classnums.append(ai)
                else:
                    pass
                    #print("doesnt exist")


    #iterate over the keypoint sequences and add them to a dataframe
    test = np.array(keypoint_sequences)
    data = {'frames': keypoint_sequences,
            'class_name' : labels,
            'class_no' : classnums
            }
    df_poses = pd.DataFrame(data)
    print("finished creating dataframe")
    return df_poses

def test_func():
    path = '/mnt/hdd/Gait/gait_dataset_keypoints/output_json_folder'
    read_pose_from_json(path, 100)



def no_pandas(path, num_user, num_session, activity_instances, camera_view, num_sequence):
    #path_string = f'output_json_folder/{user_id}/{session_id}/{activity_instance}/gait_frontview/'
    # session id starts from 1
    keypoint_sequences = []
    labels = []
    classnums = []
    for user in range(1, num_user + 1):
        for session in range(1, num_session+1):
            for ai in range(len(activity_instances)):
                user_id = 'user_'+str(user)
                session_id = 'session_' + str(session)
                path_string = f'output_json_folder/{user_id}/{session_id}/{activity_instances[ai]}/{camera_view}/'
                path_to_keypoints = path + path_string
                #print(path_to_keypoints)
                if os.path.isdir(path_to_keypoints):
                    # the path exists. try to load the keypoints
                    keypoint_frame = read_pose_from_json(path_to_keypoints, num_sequence)
                    keypoint_sequences.append(keypoint_frame)
                    labels.append(activity_instances[ai])
                    classnums.append(ai)
                else:
                    pass
                    #print("doesnt exist")


    #iterate over the keypoint sequences and add them to a dataframe
    test = np.array(keypoint_sequences)
    data = {'frames': keypoint_sequences,
            'class_name' : labels,
            'class_no' : classnums
            }
    df_poses = pd.DataFrame(data)
    print("finished creating dataframe")
    classes = df_poses.pop('class_name').unique()

    # shuffle the dataframe. this will ensure each batch in tranining loop has representation from all classes.
    # df_to_process = df_to_process.sample(frac=1).reset_index(drop=True)

    # Extract the labels
    y = classnums

    # Convert the input features and labels into the correct format for training.
    X = keypoint_sequences
    y = classnums #keras.utils.to_categorical(y)

    return X, y, classes


def find_json_files(folder):
    json_files = []
    for root, dirs, files in os.walk(folder):
        for file in files:
            if file.endswith('.json'):
                file_path = os.path.join(root, file)
                json_files.append(file_path)
    return json_files


if __name__ == '__main__':
    test_func()
